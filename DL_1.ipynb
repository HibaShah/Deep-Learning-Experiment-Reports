{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zntUrPLhrefj"
      },
      "source": [
        "import numpy as np\r\n",
        "def sigmoid(z):\r\n",
        "\r\n",
        "    s = 1/(1+np.exp(-z))\r\n",
        "    \r\n",
        "    return s\r\n",
        "\r\n",
        "def initialize_with_zeros(dim):\r\n",
        "\r\n",
        "    \r\n",
        "    w = np.zeros(shape= (dim, 1))\r\n",
        "    b = 0\r\n",
        "\r\n",
        "    assert(w.shape == (dim, 1))\r\n",
        "    assert(isinstance(b, float) or isinstance(b, int))\r\n",
        "    \r\n",
        "    return w, b\r\n",
        "def propagate(w, b, X, Y):\r\n",
        "\r\n",
        "    \r\n",
        "    m = X.shape[1]\r\n",
        "    \r\n",
        "    # FORWARD PROPAGATION (FROM X TO COST)\r\n",
        "    A = sigmoid(np.dot(X,w) +b)                                   # compute activation\r\n",
        "    cost = (-1/m)*((np.dot(np.log(A).T,Y))+(np.dot(np.log(1-A).T,1-Y)))                               # compute cost\r\n",
        "    \r\n",
        "    # BACKWARD PROPAGATION (TO FIND GRAD)\r\n",
        "    dw = 1/m*np.dot(X.T,(A-Y))\r\n",
        "    db = 1/m*np.sum(A-Y)\r\n",
        "\r\n",
        "    # print(\"w=\",w.shape,\"dw=\",dw.shape,\"A=\",A.shape,\"cost=\",cost)\r\n",
        "\r\n",
        "\r\n",
        "    assert(dw.shape == w.shape)\r\n",
        "    assert(db.dtype == float)\r\n",
        "    cost = np.squeeze(cost)\r\n",
        "    assert(cost.shape == ())\r\n",
        "    \r\n",
        "    grads = {\"dw\": dw,\r\n",
        "             \"db\": db}\r\n",
        "    \r\n",
        "    return grads, cost\r\n",
        "\r\n",
        "\r\n",
        "# function to create a list containing mini-batches by rando shuffling\r\n",
        "def create_mini_batches(X, y, batch_size): \r\n",
        "    mini_batches = [] \r\n",
        "    data = np.hstack((X, y)) \r\n",
        "    np.random.shuffle(data) \r\n",
        "    n_minibatches = data.shape[0] // batch_size \r\n",
        "    i = 0\r\n",
        "  \r\n",
        "    for i in range(n_minibatches + 1): \r\n",
        "        mini_batch = data[i * batch_size:(i + 1)*batch_size, :] \r\n",
        "        X_mini = mini_batch[:, :-1] \r\n",
        "        Y_mini = mini_batch[:, -1].reshape((-1, 1)) \r\n",
        "        mini_batches.append((X_mini, Y_mini)) \r\n",
        "    if data.shape[0] % batch_size != 0: \r\n",
        "        mini_batch = data[i * batch_size:data.shape[0]] \r\n",
        "        X_mini = mini_batch[:, :-1] \r\n",
        "        Y_mini = mini_batch[:, -1].reshape((-1, 1)) \r\n",
        "        mini_batches.append((X_mini, Y_mini))\r\n",
        "    return mini_batches\r\n",
        "\r\n",
        "def SGD(w, b, X, Y, num_iterations, learning_rate,X_val,y_val, print_cost = False,batch_size=64):\r\n",
        "\r\n",
        "    \r\n",
        "    losses = []\r\n",
        "    costs=[]\r\n",
        "    \r\n",
        "    for i in range(num_iterations):\r\n",
        "\r\n",
        "        mini_batches = create_mini_batches(X, Y, batch_size)\r\n",
        "        for mini_batch in mini_batches:\r\n",
        "        \r\n",
        "          X_mini, y_mini = mini_batch\r\n",
        "          # Cost and gradient calculation\r\n",
        "          grads, cost = propagate(w, b, X_mini, y_mini)\r\n",
        "          \r\n",
        "          # Retrieve derivatives from grads\r\n",
        "          dw = grads[\"dw\"]\r\n",
        "          db = grads[\"db\"]\r\n",
        "          \r\n",
        "          # update rule\r\n",
        "          w -= learning_rate*dw\r\n",
        "          b -= learning_rate*db\r\n",
        "        \r\n",
        "        # Record the costs\r\n",
        "        if i % 10 == 0:\r\n",
        "            costs.append(cost)\r\n",
        "\r\n",
        "\r\n",
        "        # record losses\r\n",
        "        y_hat=predict(w,b,X_val)\r\n",
        "        loss=log_loss(y_val, y_hat)\r\n",
        "        losses.append(loss)\r\n",
        "\r\n",
        "\r\n",
        "        # Print the cost every 100 training iterations\r\n",
        "        if print_cost and i % 10 == 0:\r\n",
        "            print (\"cost after iteration %i: %f\" %(i, cost))\r\n",
        "    \r\n",
        "    params = {\"w\": w,\r\n",
        "              \"b\": b}\r\n",
        "    \r\n",
        "    grads = {\"dw\": dw,\r\n",
        "             \"db\": db}\r\n",
        "    \r\n",
        "    return params, grads, costs,losses\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def GD(w, b, X, Y, num_iterations, learning_rate,X_val,y_val, print_cost = False):\r\n",
        "\r\n",
        "    losses = []\r\n",
        "    costs = []\r\n",
        "    \r\n",
        "    for i in range(num_iterations):\r\n",
        "\r\n",
        "        \r\n",
        "        \r\n",
        "        # Cost and gradient calculation\r\n",
        "        grads, cost = propagate(w, b, X, Y)\r\n",
        "        \r\n",
        "        # Retrieve derivatives from grads\r\n",
        "        dw = grads[\"dw\"]\r\n",
        "        db = grads[\"db\"]\r\n",
        "        \r\n",
        "        # update rule\r\n",
        "        w -= learning_rate*dw\r\n",
        "        b -= learning_rate*db\r\n",
        "        \r\n",
        "        # Record the costs\r\n",
        "        if i % 10 == 0:\r\n",
        "            costs.append(cost)\r\n",
        "\r\n",
        "        # record losses\r\n",
        "        y_hat=predict(w,b,X_val)\r\n",
        "        loss=log_loss(y_val, y_hat)\r\n",
        "        losses.append(loss)\r\n",
        "        \r\n",
        "        # Print the cost every 100 training iterations\r\n",
        "        if print_cost and i % 10 == 0:\r\n",
        "            print (\"Cost after iteration %i: %f\" %(i, cost))\r\n",
        "    \r\n",
        "    params = {\"w\": w,\r\n",
        "              \"b\": b}\r\n",
        "    \r\n",
        "    grads = {\"dw\": dw,\r\n",
        "             \"db\": db}\r\n",
        "    \r\n",
        "    return params, grads, costs, losses\r\n",
        "\r\n",
        "def predict(w, b, X):\r\n",
        "\r\n",
        "    \r\n",
        "    m = X.shape[0]\r\n",
        "    Y_prediction = np.zeros((m,1))\r\n",
        "    w = w.reshape(X.shape[1], 1)\r\n",
        "    \r\n",
        "    # Compute vector \"A\" predicting the probabilities\r\n",
        "    A = sigmoid(np.dot(X,w) + b)\r\n",
        "    \r\n",
        "    for i in range(A.shape[0]):\r\n",
        "        \r\n",
        "        # Convert probabilities A[0,i] to actual predictions p[0,i]\r\n",
        "        if A[i,0] > 0.5:\r\n",
        "            Y_prediction[i,0] = 1\r\n",
        "        else:\r\n",
        "            Y_prediction[i,0] = 0\r\n",
        "    \r\n",
        "    assert(Y_prediction.shape == (m, 1))\r\n",
        "    \r\n",
        "    return Y_prediction\r\n",
        "\r\n",
        "def model(X_train, Y_train, X_test, Y_test, num_iterations = 900, learning_rate = 0.05, print_cost = False, optimizer=\"SGD\"):\r\n",
        "\r\n",
        "    Y_train=Y_train.reshape(-1,1)\r\n",
        "    Y_test=Y_test.reshape(-1,1)\r\n",
        "    # initialize parameters with zeros\r\n",
        "    w, b = initialize_with_zeros(X_train.shape[1])\r\n",
        "\r\n",
        "    if optimizer==\"SGD\":\r\n",
        "      # Gradient descent\r\n",
        "      parameters, grads, costs, losses = SGD(w , b , X_train , Y_train , num_iterations , learning_rate ,\r\n",
        "                                            X_test,Y_test,print_cost)\r\n",
        "    else:\r\n",
        "      parameters, grads, costs, losses = GD(w , b , X_train , Y_train , num_iterations , learning_rate ,\r\n",
        "                                            X_test,Y_test,print_cost)\r\n",
        "    \r\n",
        "    # Retrieve parameters w and b from dictionary \"parameters\"\r\n",
        "    w = parameters[\"w\"]\r\n",
        "    b = parameters[\"b\"]\r\n",
        "    \r\n",
        "    # Predict test/train set examples\r\n",
        "    Y_prediction_test = predict(w , b , X_test)\r\n",
        "    Y_prediction_train = predict(w , b, X_train)\r\n",
        "\r\n",
        "    ### END CODE HERE ###\r\n",
        "\r\n",
        "    # Print train/test Errors\r\n",
        "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\r\n",
        "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\r\n",
        "    print(\"Total validation loss: {}\".format(log_loss(Y_test, Y_prediction_test)))\r\n",
        "\r\n",
        "    \r\n",
        "    d = {\"costs\": costs,\r\n",
        "         \"Y_prediction_test\": Y_prediction_test, \r\n",
        "         \"Y_prediction_train\" : Y_prediction_train, \r\n",
        "         \"w\" : w, \r\n",
        "         \"b\" : b,\r\n",
        "         \"learning_rate\" : learning_rate,\r\n",
        "         \"num_iterations\": num_iterations,\r\n",
        "         \"losses\": losses}\r\n",
        "    \r\n",
        "    return d"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSwgulqmsUQR"
      },
      "source": [
        "from sklearn.metrics import log_loss\r\n",
        "import numpy.random as rn\r\n",
        "import numpy.linalg as la\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import math\r\n",
        "import csv\r\n",
        "import time\r\n",
        "from sklearn import preprocessing\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import h5py\r\n",
        "import scipy\r\n",
        "import numpy as np\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "import pandas as pd\r\n",
        "from random import shuffle"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Zf9aBFCjONS"
      },
      "source": [
        "**Reading Data set pre saved into csv format**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aq2_o5z5sXid",
        "outputId": "dff2a59f-88b1-4546-8a50-23e4ea183a92"
      },
      "source": [
        "dt=pd.read_csv('data_1.csv')\r\n",
        "dt.shape"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(32560, 123)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TapbR_U-jT61"
      },
      "source": [
        "**Converting labels into 0 and 1 for our own convenience they can be retrasfoemd at the end if we want**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPjFxJN8s8pU",
        "outputId": "e1c89da8-c9ae-4632-f314-d9497c0a406d"
      },
      "source": [
        "le = preprocessing.LabelEncoder()\r\n",
        "le.fit(dt[\"target\"])\r\n",
        "y=le.transform(dt[\"target\"])\r\n",
        "y.shape"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(32560,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFnhlbVas_YG",
        "outputId": "0a008f18-a871-4a9c-c773-c7261caead9c"
      },
      "source": [
        "# splitting data into train and test set by randomly shuffling\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(dt.iloc[:,1:], y, test_size=0.33)\r\n",
        "print(X_train.shape,y_train.shape)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(21815, 122) (21815,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zztZHqLitAoc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd37d9f2-0210-40f6-b6d2-ecedf49d48cd"
      },
      "source": [
        "result=model(X_train.values, y_train, X_test.values, y_test, num_iterations = 100, learning_rate = 0.005, print_cost = True, optimizer=\"SGD\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cost after iteration 0: 0.236143\n",
            "cost after iteration 10: 0.233998\n",
            "cost after iteration 20: 0.162300\n",
            "cost after iteration 30: 0.151119\n",
            "cost after iteration 40: 0.112193\n",
            "cost after iteration 50: 0.175938\n",
            "cost after iteration 60: 0.145338\n",
            "cost after iteration 70: 0.197125\n",
            "cost after iteration 80: 0.163579\n",
            "cost after iteration 90: 0.101184\n",
            "train accuracy: 83.86431354572542 %\n",
            "test accuracy: 84.16007445323406 %\n",
            "Total validation loss: 5.470955608411938\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "i1SI2Niof-ko",
        "outputId": "d68bc543-328e-45ca-bd67-6a82d7a4db3f"
      },
      "source": [
        "# curve for loss validation with respect to iterations FOR GRADIENT DESCENT\r\n",
        "plt.plot(list(range(len(result[\"losses\"]))), result[\"losses\"], '-r')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fbd657fffd0>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}